{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRÁCTICA II: Determinación de la apertura de una ventana motorizada\n",
    "\n",
    "En esta práctica modificaremos una grabación de una ventana motorizada con la ayuda de las distintas técnicas vistas en los temas 1-4 para conseguir determinar qué porcentaje de dicha ventana está abierta en distintas situaciones visuales (mucha iluminación, oscuridad, partículas, etc.)\n",
    "\n",
    "Además el usuario que utilice el programa deberá seleccionar 4 puntos en sentido horario que definirán las esquinas de la ventana para generar una imagen transformada de ésta (Aclaración: se debe evitar introducir marcos de la ventana en la imagen segmentada para no influir en el resultado del algoritmo).\n",
    "\n",
    "Video con el resultado final en: PracticaXanelaCV>DATA>Imagen_resultado.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACLARACIÓN IMPORTANTE: Primero se explicarán los distintos segmentos del código de manera no funcional y al final del notebook se añadirá el código ejecutable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerías necesarias para manipular los frames de la imagen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. En primer lugar leeremos el video aportado y seleccionaremos el primer frame para que el usuario seleccione los 4 vértices de la ventana.\n",
    "Para ello utilizamos la función de OpenCV cv2.setMouseCallback() que invocará el callback 'pick_points()' el cual introducirá en una lista global los puntos seleccionados en pantalla en orden horario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('./PracticaXanelaCV/DATA/proba.mp4')\n",
    "if not cap.isOpened():\n",
    "    print(\"Error al abrir el video.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener las coordenadas (x,y) donde se hace click con el ratón\n",
    "pts = []\n",
    "def pick_points(event,x,y,flags,param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        pts.append((x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, image = cap.read()\n",
    "if(ret):\n",
    "    cv2.imshow('Primer fotograma', image)\n",
    "    while(len(pts) < 4):\n",
    "        cv2.setMouseCallback('Primer fotograma', pick_points)\n",
    "        # Mostramos los puntos seleccionados\n",
    "        if(len(pts) > 0):\n",
    "            cv2.circle(image, pts[len(pts)-1], 5, (0,0,255), -1)\n",
    "            cv2.imshow('Primer fotograma', image)\n",
    "\n",
    "        if (cv2.waitKey(30) == ord('q')):\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reordenamos los 4 puntos seleccionados de la lista para crear la imagen transformada de todos los frames del video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corregimos la distorsión de la ventana\n",
    "pts_src = np.array([pts[0], pts[3], pts[1], pts[2]], np.float32)\n",
    "pts_dst = np.array([(0,0), (0,image.shape[0]), (image.shape[1],0), (image.shape[1],image.shape[0])], np.float32)\n",
    "matrix = cv2.getPerspectiveTransform(pts_src, pts_dst)\n",
    "trans_image = cv2.warpPerspective(image, matrix, (image.shape[1],image.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Edición de los frames\n",
    "### 3.1 Recorte de ventana\n",
    "Para reducir la posibilidad de obtener datos con ruido se optó por dividir la imagen de la ventana en 3 segmentos verticales iguales y aplicar únicamente aplicar filtros a la división central."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recortamos la imagen\n",
    "cropped_image = trans_image[0:trans_image.shape[0], trans_image.shape[1]//3:trans_image.shape[1]//3*2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Modificación de la imagen\n",
    "- Escala de grises -> Como el color no nos aporta nada para el algoritmo solicitado trabajaremos con una imagen en escala de grises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2GRAY)\n",
    "mean_gray = np.mean(gray_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcularemos la media de valores de gris para posteriormente diferenciar entre frames con mucha intensidad de luz (día) o poca (noche)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos el valor de la media de grises de la imagen para determinar si es de dia o de noche\n",
    "if(mean_gray>=150):\n",
    "    # Filtros diurnos\n",
    "    blurred_image = cv2.GaussianBlur(gray_image, ksize=(31, 1), sigmaX=0)                       # Emborronamos la imagen con un kernel orientado en el eje X\n",
    "    threshold = cv2.inRange(blurred_image, lowerb=90, upperb=205)                               # Filtramos los valores de grises\n",
    "    cannyed_image = cv2.Canny(threshold, threshold1=350, threshold2=220)                        # Obtenemos los bordes\n",
    "    dilated_image = cv2.dilate(cannyed_image, kernel=np.ones((3,3), np.uint8), iterations=1)    # Dilatamos las lineas de los bordes obtenidos para unificar lineas horizontales\n",
    "\n",
    "    # Creamos las lineas de la transformada de Hough\n",
    "    hough_lines = cv2.HoughLinesP(dilated_image, rho = 1, theta=np.pi/180, threshold = 60, minLineLength = 150, maxLineGap = 50)\n",
    "else:\n",
    "    # Filtros nocturnos\n",
    "    blurred_image = cv2.GaussianBlur(gray_image, ksize=(71, 1), sigmaX=0)                       # Emborronamos la imagen con un kernel orientado en el eje X\n",
    "    threshold = cv2.inRange(blurred_image, lowerb=70, upperb=110)                               # Filtramos los valores de grises\n",
    "    eroded_image = cv2.erode(threshold, kernel=np.ones((1,30), np.uint8), iterations=3)         # Erosionamos en el eje X\n",
    "    ret, mask = cv2.threshold(eroded_image, thresh=180, maxval=255, type=cv2.THRESH_BINARY)     # Conversion a binario\n",
    "    cannyed_image = cv2.Canny(mask, threshold1=350, threshold2=220)                             # Obtenemos los bordes\n",
    "    dilated_image = cv2.dilate(cannyed_image, kernel=np.ones((3,3), np.uint8), iterations=1)    # Dilatamos las lineas de los bordes obtenidos para unificar lineas horizontales\n",
    "    blurred_image2 = cv2.GaussianBlur(dilated_image, ksize=(31, 1), sigmaX=0)                   # Emborronamos la imagen una vez más para reducir el ruido\n",
    "\n",
    "    # Creamos las lineas de la transformada de Hough\n",
    "    hough_lines = cv2.HoughLinesP(blurred_image2, rho = 1, theta=np.pi/180, threshold = 60, minLineLength = 150, maxLineGap = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Unificar las lineas de Hough obtenidas y visualizarlas\n",
    "### 4.1 Separamos las lineas de la transformada de Hough en eje X e eje Y\n",
    "Para ello se creó la función 'separate_x_y()' que devuelve 2 listas con los respectivos valores de X e Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para separar en listas distintas los valores de las coordenadas 'x' e 'y' de las lineas dadas\n",
    "def separate_x_y(lines):\n",
    "    lines_x = []\n",
    "    lines_y = []\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            for x1, y1, x2, y2 in line:\n",
    "                lines_x.extend([x1, x2])\n",
    "                lines_y.extend([y1, y2])\n",
    "\n",
    "    return lines_x, lines_y\n",
    "\n",
    "lines_x, lines_y = separate_x_y(hough_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Clasificamos los valores del eje Y para diferenciar entre borde superior e inferior de la barra\n",
    "Recorremos la lista de valores 'lines_y' y diferenciamos entre valores del borde superior del inferior de la barra utilizando el valor mínimo de dicha lista (cuanto más pequeño sea el valor, más arriba en la imagen está situado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_y = []\n",
    "bottom_y = []\n",
    "for val in lines_y:\n",
    "    if(min(lines_y) <= val <= min(lines_y)+10):\n",
    "        # Valores de la horizontal superior\n",
    "        top_y.append(val)\n",
    "    else:\n",
    "        # Valores de la horizontal inferior\n",
    "        bottom_y.append(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Unificamos los valores del eje Y para los dos bordes utilizando la media\n",
    "Una vez obtenidos todos los valores para el borde superior y el inferior, calculamos la media de los valores para unificar todas los bordes obtenidos en uno solo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(len(top_y)!=0): line_y_top = round(np.mean(top_y))\n",
    "else: line_y_top = 0\n",
    "if(len(bottom_y)!=0): line_y_bottom = round(np.mean(bottom_y))\n",
    "else: line_y_bottom = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Obtenemos el valor mínimo y máximo para los ejes X e Y y creamos los rectángulos que representan las porciones de ventana cerrada y abierta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_x = 0\n",
    "max_x = trans_image.shape[1]\n",
    "min_y = 0\n",
    "max_y = trans_image.shape[0]\n",
    "\n",
    "square_top = [[[min_x, 0, max_x, 0], [max_x, 0, max_x, line_y_top], [min_x, line_y_top, max_x, line_y_top], [min_x, 0, min_x, line_y_top]]]\n",
    "square_bottom = [[[min_x, line_y_bottom, max_x, line_y_bottom], [max_x, line_y_bottom, max_x, max_y], [min_x, max_y, max_x, max_y], [min_x, line_y_bottom, min_x, max_y]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Dibujamos los rectángulos en la imagen inicial\n",
    "Para ello creamos la función 'draw_lines()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para dibujar lineas en la imagen dada\n",
    "def draw_lines(img, lines, color = [0, 0, 255], thickness = 4):\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            for x1,y1,x2,y2 in line:\n",
    "                cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n",
    "\n",
    "lines_image = np.zeros((max_y, max_x, 3), dtype = np.uint8)\n",
    "if(line_y_top!=0):\n",
    "    draw_lines(lines_image, square_top, color=[255,0,0])\n",
    "if(line_y_bottom!=0):\n",
    "    draw_lines(lines_image, square_bottom, color=[0,0,255])\n",
    "final_image = cv2.addWeighted(trans_image, alpha = 0.7, src2=lines_image, beta = 1.0, gamma = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculamos los porcentajes de apertura y cierre de la ventana y los visualizamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos el porcentaje de apertura y cierre de la cortina\n",
    "if(line_y_top!=0 and line_y_bottom!=0):\n",
    "    line_center = (line_y_top+line_y_bottom)/2\n",
    "    percent_top = round(line_center/max_y*100, 2)\n",
    "    percent_bottom = round(100-percent_top, 2)\n",
    "elif(line_y_top!=0 and line_y_bottom==0):\n",
    "    percent_top = 100\n",
    "    percent_bottom = 0\n",
    "elif(line_y_top==0 and line_y_bottom!=0):\n",
    "    percent_top = 0\n",
    "    percent_bottom = 100\n",
    "else:\n",
    "    percent_top = None\n",
    "    percent_bottom = None\n",
    "\n",
    "# Agregamos texto a la imagen\n",
    "cv2.putText(final_image, 'Cerrada: '+str(percent_top)+'%', org=(5, 30), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=[255, 0, 0], thickness=2, lineType=1, bottomLeftOrigin=False)\n",
    "cv2.putText(final_image, 'Abierta: '+str(percent_bottom)+'%', org=(5, 60), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=[0, 0, 255], thickness=2, lineType=1, bottomLeftOrigin=False)\n",
    "\n",
    "# Visualizacion\n",
    "cv2.imshow('Imagen Final', final_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSIÓN FINAL\n",
    "Podemos comprobar que dependiendo de las condiciones medioambientales (poca/mucha intensidad de luz, partículas en el aire (lluvia, polvo), etc.) el ruido de la imagen de entrada varía, por lo tanto debemos de tener esto en cuenta para conseguir una solución global a todas las situaciones posibles y dar un resultado lo más fiable posible. Por ejemplo, el código creado funciona mejor con buena iluminación y sin partículas que cuando la imagen presenta partículas (lluvia o polvo) ya que éstas distorsionan la imagen de la barra produciendo ruido en la imagen de salida y obteniendo así una medición de los porcentajes de apertura/cierre menos exacta.\n",
    "\n",
    "En esta práctica se optó por diferenciar las grabaciones diurnas de las nocturnas mediante el valor medio de la escala de grises, para así aplicar distintas combinaciones de filtros a cada una. Además, para obtener una buena respuesta del filtro de Canny tuvimos que preprocesar la imagen que le ibamos a pasar para evitar falsos positivos. \n",
    "Una de las mayores ventajas de esta práctica es que el objeto a detectar es completamente horizontal, por lo que se aprovechó mucho el uso del emborronamiento Gaussiano con un kernel orientado en el eje X para eliminar la mayor parte de las detecciones que no nos interesaban."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CÓDIGO EJECUTABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "\n",
    "\n",
    "# Función para obtener las coordenadas (x,y) donde se hace click con el ratón\n",
    "pts = []\n",
    "def pick_points(event,x,y,flags,param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        pts.append((x,y))\n",
    "\n",
    "# Función para separar en listas distintas los valores de las coordenadas 'x' e 'y' de las lineas dadas\n",
    "def separate_x_y(lines):\n",
    "    lines_x = []\n",
    "    lines_y = []\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            for x1, y1, x2, y2 in line:\n",
    "                lines_x.extend([x1, x2])\n",
    "                lines_y.extend([y1, y2])\n",
    "\n",
    "    return lines_x, lines_y\n",
    "\n",
    "# Función para dibujar lineas en la imagen dada\n",
    "def draw_lines(img, lines, color = [0, 0, 255], thickness = 4):\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            for x1,y1,x2,y2 in line:\n",
    "                cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n",
    "\n",
    "\n",
    "# MAIN():\n",
    "cap = cv2.VideoCapture('./PracticaXanelaCV/DATA/proba.mp4')\n",
    "if not cap.isOpened():\n",
    "    print(\"Error al abrir el video.\")\n",
    "    exit()\n",
    "\n",
    "ret, image = cap.read()\n",
    "if(ret):\n",
    "    cv2.imshow('Primer fotograma', image)\n",
    "    while(len(pts) < 4):\n",
    "        cv2.setMouseCallback('Primer fotograma', pick_points)\n",
    "        # Mostramos los puntos seleccionados\n",
    "        if(len(pts) > 0):\n",
    "            cv2.circle(image, pts[len(pts)-1], 5, (0,0,255), -1)\n",
    "            cv2.imshow('Primer fotograma', image)\n",
    "\n",
    "        if (cv2.waitKey(30) == ord('q')):\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "while(ret):\n",
    "    ret, image = cap.read()\n",
    "    if(not ret):\n",
    "        break\n",
    "\n",
    "    # Corregimos la distorsión de la ventana\n",
    "    pts_src = np.array([pts[0], pts[3], pts[1], pts[2]], np.float32)\n",
    "    pts_dst = np.array([(0,0), (0,image.shape[0]), (image.shape[1],0), (image.shape[1],image.shape[0])], np.float32)\n",
    "    matrix = cv2.getPerspectiveTransform(pts_src, pts_dst)\n",
    "    trans_image = cv2.warpPerspective(image, matrix, (image.shape[1],image.shape[0]))\n",
    "\n",
    "    # Recortamos la imagen\n",
    "    cropped_image = trans_image[0:trans_image.shape[0], trans_image.shape[1]//3:trans_image.shape[1]//3*2]\n",
    "    \n",
    "    # Aplicamos filtros:\n",
    "    gray_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2GRAY)\n",
    "    mean_gray = np.mean(gray_image)\n",
    "\n",
    "    # Obtenemos el valor de la media de grises de la imagen para determinar si es de dia o de noche\n",
    "    if(mean_gray>=150):\n",
    "        # Filtros diurnos\n",
    "        blurred_image = cv2.GaussianBlur(gray_image, ksize=(31, 1), sigmaX=0)                       # Emborronamos la imagen con un kernel orientado en el eje X\n",
    "        threshold = cv2.inRange(blurred_image, lowerb=90, upperb=205)                               # Filtramos los valores de grises\n",
    "        cannyed_image = cv2.Canny(threshold, threshold1=350, threshold2=220)                        # Obtenemos los bordes\n",
    "        dilated_image = cv2.dilate(cannyed_image, kernel=np.ones((3,3), np.uint8), iterations=1)    # Dilatamos las lineas de los bordes obtenidos para unificar lineas horizontales\n",
    "\n",
    "        # Creamos las lineas de la transformada de Hough\n",
    "        hough_lines = cv2.HoughLinesP(dilated_image, rho = 1, theta=np.pi/180, threshold = 60, minLineLength = 150, maxLineGap = 50)\n",
    "    else:\n",
    "        # Filtros nocturnos\n",
    "        blurred_image = cv2.GaussianBlur(gray_image, ksize=(71, 1), sigmaX=0)                       # Emborronamos la imagen con un kernel orientado en el eje X\n",
    "        threshold = cv2.inRange(blurred_image, lowerb=70, upperb=110)                               # Filtramos los valores de grises\n",
    "        eroded_image = cv2.erode(threshold, kernel=np.ones((1,30), np.uint8), iterations=3)         # Erosionamos en el eje X\n",
    "        ret, mask = cv2.threshold(eroded_image, thresh=180, maxval=255, type=cv2.THRESH_BINARY)     # Conversion a binario\n",
    "        cannyed_image = cv2.Canny(mask, threshold1=350, threshold2=220)                             # Obtenemos los bordes\n",
    "        dilated_image = cv2.dilate(cannyed_image, kernel=np.ones((3,3), np.uint8), iterations=1)    # Dilatamos las lineas de los bordes obtenidos para unificar lineas horizontales\n",
    "        blurred_image2 = cv2.GaussianBlur(dilated_image, ksize=(31, 1), sigmaX=0)                   # Emborronamos la imagen una vez más para reducir el ruido\n",
    "\n",
    "        # Creamos las lineas de la transformada de Hough\n",
    "        hough_lines = cv2.HoughLinesP(blurred_image2, rho = 1, theta=np.pi/180, threshold = 60, minLineLength = 150, maxLineGap = 50)\n",
    "\n",
    "    # Juntamos todas las lineas horizontales en una única para cada borde de la barra (una horizontal para superior, otra horizontal para el inferior)\n",
    "    lines_x, lines_y = separate_x_y(hough_lines)\n",
    "\n",
    "    top_y = []\n",
    "    bottom_y = []\n",
    "    for val in lines_y:\n",
    "        if(min(lines_y) <= val <= min(lines_y)+15):\n",
    "            # Valores de la horizontal superior\n",
    "            top_y.append(val)\n",
    "        else:\n",
    "            # Valores de la horizontal inferior\n",
    "            bottom_y.append(val)\n",
    "\n",
    "    if(len(top_y)!=0): line_y_top = round(np.mean(top_y))\n",
    "    else: line_y_top = 0\n",
    "    if(len(bottom_y)!=0): line_y_bottom = round(np.mean(bottom_y))\n",
    "    else: line_y_bottom = 0\n",
    "\n",
    "    # Definimos las lineas y las agregamos a la imagen inicial\n",
    "    min_x = 0\n",
    "    max_x = trans_image.shape[1]\n",
    "    min_y = 0\n",
    "    max_y = trans_image.shape[0]\n",
    "\n",
    "    square_top = [[[min_x, 0, max_x, 0], [max_x, 0, max_x, line_y_top], [min_x, line_y_top, max_x, line_y_top], [min_x, 0, min_x, line_y_top]]]\n",
    "    square_bottom = [[[min_x, line_y_bottom, max_x, line_y_bottom], [max_x, line_y_bottom, max_x, max_y], [min_x, max_y, max_x, max_y], [min_x, line_y_bottom, min_x, max_y]]]\n",
    "\n",
    "    lines_image = np.zeros((max_y, max_x, 3), dtype = np.uint8)\n",
    "    if(line_y_top!=0):\n",
    "        draw_lines(lines_image, square_top, color=[255,0,0])\n",
    "    if(line_y_bottom!=0):\n",
    "        draw_lines(lines_image, square_bottom, color=[0,0,255])\n",
    "    final_image = cv2.addWeighted(trans_image, alpha = 0.7, src2=lines_image, beta = 1.0, gamma = 0.0)\n",
    "\n",
    "    # Calculamos el porcentaje de apertura y cierre de la cortina\n",
    "    if(line_y_top!=0 and line_y_bottom!=0):\n",
    "        line_center = (line_y_top+line_y_bottom)/2\n",
    "        percent_top = round(line_center/max_y*100, 2)\n",
    "        percent_bottom = round(100-percent_top, 2)\n",
    "    elif(line_y_top!=0 and line_y_bottom==0):\n",
    "        percent_top = 100\n",
    "        percent_bottom = 0\n",
    "    elif(line_y_top==0 and line_y_bottom!=0):\n",
    "        percent_top = 0\n",
    "        percent_bottom = 100\n",
    "    else:\n",
    "        percent_top = None\n",
    "        percent_bottom = None\n",
    "\n",
    "    # Agregamos texto a la imagen\n",
    "    cv2.putText(final_image, 'Cerrada: '+str(percent_top)+'%', org=(5, 30), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=[255, 0, 0], thickness=2, lineType=1, bottomLeftOrigin=False)\n",
    "    cv2.putText(final_image, 'Abierta: '+str(percent_bottom)+'%', org=(5, 60), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=[0, 0, 255], thickness=2, lineType=1, bottomLeftOrigin=False)\n",
    "\n",
    "    # Visualizacion\n",
    "    cv2.imshow('Imagen Final', final_image)\n",
    "\n",
    "    if cv2.waitKey(20) == ord('q'):\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VAA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
