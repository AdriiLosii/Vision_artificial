{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESTÁTICA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Función para calcular el mapa de saliencia basado en el espectro de fase de la transformada de Fourier\n",
    "def PFT_Color_Multiescala(img):\n",
    "    # Si es una imagen en escala de grises, no podemos obtener información de los colores \n",
    "    if len(img.shape) == 2:\n",
    "        print(\"Imagen en escala de grises, devolviendo la imagen original...\")\n",
    "        return img\n",
    "\n",
    "    rows, cols = img.shape[0], img.shape[1]\n",
    "\n",
    "\n",
    "    # Pasamos de BGR a LAB\n",
    "    imageLAB = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # Obtenemos los canales L->blanco-negro (luminosidad), A->rojo-verde, B->azul-amarillo\n",
    "    L = imageLAB[:,:,0]\n",
    "    A = imageLAB[:,:,1]\n",
    "    B = imageLAB[:,:,2]\n",
    "    channels = [L, A, B]\n",
    "    results = []\n",
    "\n",
    "    # Para cada canal\n",
    "    for i in range(len(channels)):\n",
    "        # Transformada de Fourier\n",
    "        img_fft = np.fft.fft2(channels[i])\n",
    "\n",
    "        # Espectro de la fase de la imagen\n",
    "        phase_espectrum = np.angle(img_fft)\n",
    "\n",
    "        # Mapa de saliencia\n",
    "        sm = cv2.GaussianBlur((np.abs(np.fft.ifft2(np.exp(1j * phase_espectrum)))**2), (3, 3), 1)\n",
    "\n",
    "        # Redimensionamos y normalizamos el mapa de saliencia\n",
    "        sm = cv2.normalize(cv2.resize(sm, (rows, cols), cv2.INTER_LANCZOS4), None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "        # Calcular la varianza del mapa de saliencia del canal actual\n",
    "        var = np.var(sm)\n",
    "\n",
    "        # Ponderar el mapa de saliencia del canal actual en función de la varianza\n",
    "        results.append(sm / (var + 1e-8))   # Se agrega un pequeño valor para evitar la división por cero\n",
    "\n",
    "    # Normalizar las ponderaciones para que sumen 1\n",
    "    weights = np.array(results) / np.sum(results, axis=0)\n",
    "\n",
    "    # Sumar los mapas ponderados\n",
    "    combined_result = np.sum(weights * np.array(results), axis=0)\n",
    "\n",
    "    return combined_result\n",
    "\n",
    "\n",
    "# Cargar video\n",
    "video_path = \"./videos/SCE_bur_1_distractores_altos.avi\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Obtener información del video (dimensiones y fps)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Iterar sobre los fotogramas del video\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Llamamos a la función para calcular el mapa de saliencia a multiescala con información de color e invariante a la escala\n",
    "    sm = PFT_Color_Multiescala(frame)\n",
    "\n",
    "    # Visualización en tiempo real (opcional)\n",
    "    cv2.imshow('Salient Region', sm)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberar objetos y cerrar ventanas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PFT_generalizado(frame):\n",
    "    ############ CONTINUAR ##################\n",
    "    rows, cols = frame.shape[0], frame.shape[1]\n",
    "\n",
    "    # Transformada de Fourier\n",
    "    img_fft = np.fft.fft2(frame)\n",
    "\n",
    "    # Espectro de la fase de la imagen\n",
    "    phase_espectrum = np.angle(img_fft)\n",
    "\n",
    "    # Mapa de saliencia\n",
    "    sm = cv2.GaussianBlur((np.abs(np.fft.ifft2(np.exp(1j * phase_espectrum)))**2), (3, 3), 1)\n",
    "\n",
    "    # Redimensionamos y normalizamos el mapa de saliencia\n",
    "    sm = cv2.normalize(cv2.resize(sm, (rows, cols), cv2.INTER_LANCZOS4), None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "    return sm\n",
    "\n",
    "def prueba(prev_frame, current_frame):\n",
    "    b0, g0, r0 = cv2.split(prev_frame)\n",
    "    b, g, r = cv2.split(current_frame)\n",
    "\n",
    "    # Colores\n",
    "    R = r - (g + b)/2\n",
    "    G = g - (r + b)/2\n",
    "    B = b - (r + g)/2\n",
    "    Y = (r + g)/2 - np.abs(r - g)/2 - b\n",
    "\n",
    "    # Canales componente-opuesta de color\n",
    "    RG = R - G\n",
    "    BY = B - Y\n",
    "\n",
    "    # Canal de intensidad\n",
    "    I = (r + g + b)/3\n",
    "    I_tau = (r0 + g0 + b0)/3\n",
    "\n",
    "    # Canal de movimiento\n",
    "    M = np.abs(I - I_tau)   # Tau = 1\n",
    "\n",
    "    # Devolvemos los 4 canales: 2 de colores, 1 de intensidad y 1 de movimiento\n",
    "    return RG, BY, I, M\n",
    "\n",
    "\n",
    "# Función para calcular la saliencia basada en color utilizando el modelo Itti-Koch y luego invertir los colores\n",
    "def color_SM(frame):\n",
    "    # Convertir el fotograma a espacio de color LAB\n",
    "    lab_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # Separar los canales LAB\n",
    "    l, a, b = cv2.split(lab_frame)\n",
    "\n",
    "    # Normalizar los canales a valores entre 0 y 1\n",
    "    l_normalized = cv2.normalize(l, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "    a_normalized = cv2.normalize(a, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "    b_normalized = cv2.normalize(b, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "    # Calcular la saliencia basada en todos los canales LAB\n",
    "    sm = cv2.GaussianBlur(l_normalized, (0, 0), sigmaX=2, sigmaY=2)\n",
    "    sm += cv2.GaussianBlur(a_normalized, (0, 0), sigmaX=2, sigmaY=2)\n",
    "    sm += cv2.GaussianBlur(b_normalized, (0, 0), sigmaX=2, sigmaY=2)\n",
    "\n",
    "    # Escalar a valores entre 0 y 255 y luego invertir los colores\n",
    "    sm = ((255 - (sm * 255 / np.max(sm))).astype(np.uint8))\n",
    "\n",
    "    # Normalizamos los valores\n",
    "    sm_norm = cv2.normalize(sm,None,0.0,255,cv2.NORM_MINMAX, cv2.CV_32F)\n",
    "\n",
    "    # Mapa de calor\n",
    "    sm_heated = cv2.applyColorMap(np.uint8(sm_norm), cv2.COLORMAP_JET)\n",
    "\n",
    "    return sm_heated\n",
    "\n",
    "# Función para calcular la saliencia basada en movimiento\n",
    "def motion_SM(prev_frame, current_frame):\n",
    "    # Convertir los fotogramas a escala de grises\n",
    "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    current_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calcular el flujo óptico usando el método Farneback\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_gray, current_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "    # Calcular la magnitud del flujo óptico\n",
    "    magnitude = np.sqrt(flow[:, :, 0] ** 2 + flow[:, :, 1] ** 2)\n",
    "\n",
    "    # Normalizar la saliencia de movimiento\n",
    "    sm = magnitude / np.max(magnitude)\n",
    "\n",
    "    # Normalizamos los valores\n",
    "    sm_norm = cv2.normalize(sm,None,0.0,255,cv2.NORM_MINMAX, cv2.CV_32F)\n",
    "\n",
    "    # Mapa de calor\n",
    "    sm_heated = cv2.applyColorMap(np.uint8(sm_norm), cv2.COLORMAP_JET)\n",
    "\n",
    "    return sm_heated\n",
    "\n",
    "# Función para combinar el mapa de saliencia con información de color y con movimiento\n",
    "def GPhase_color_SM(color_sm, motion_sm):\n",
    "    return (color_sm + motion_sm) / 2.0\n",
    "\n",
    "\n",
    "\n",
    "# Leer el video\n",
    "video = cv2.VideoCapture('./videos/SCE_bur_1_distractores_altos.avi')\n",
    "\n",
    "# Leer el primer fotograma\n",
    "ret, prev_frame = video.read()\n",
    "#display_handle = display(None, display_id=True)\n",
    "while True:\n",
    "    ret, current_frame = video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Calcular la saliencia basada en color y movimiento\n",
    "    color_sm = color_SM(current_frame)\n",
    "    motion_sm = motion_SM(prev_frame, current_frame)\n",
    "    combined_sm = GPhase_color_SM(color_sm, motion_sm)\n",
    "\n",
    "    # Convertir color_sm y motion_sm a float64\n",
    "    color_sm = color_sm.astype('float64')\n",
    "    motion_sm = motion_sm.astype('float64')\n",
    "\n",
    "    # Visualización del mapa saliencia mediante color, mediante movimiento y ambos combinados\n",
    "    subplot = cv2.hconcat((color_sm, motion_sm, combined_sm))\n",
    "    ret, subplot = cv2.imencode('.jpg', subplot)\n",
    "    #display_handle.update(Image(data=subplot.tobytes()))\n",
    "    RG, BY, I, M = prueba(prev_frame, current_frame)\n",
    "    channels = [RG, BY, I, M]\n",
    "    results = []\n",
    "\n",
    "    for c in channels:\n",
    "        var = np.var(c)\n",
    "        results.append(c / (var + 1e-8))\n",
    "\n",
    "    weights = np.array(results) / np.sum(results, axis=0)\n",
    "    combined_result = np.sum(weights * np.array(results), axis=0)\n",
    "\n",
    "    cv2.imshow(\"RG\", RG)\n",
    "    cv2.imshow(\"BY\", BY)\n",
    "    cv2.imshow(\"I\", I)\n",
    "    cv2.imshow(\"M\", M)\n",
    "    cv2.imshow(\"Combined\", combined_result)\n",
    "    cv2.waitKey(10)\n",
    "    \"\"\"cv2.imshow(\"Color\", color_sm)\n",
    "    cv2.imshow(\"Motion\", motion_sm)\n",
    "    cv2.imshow(\"Color+Motion\", combined_sm)\"\"\"\n",
    "\n",
    "    if cv2.waitKey(20) == ord('q'):\n",
    "        break\n",
    "\n",
    "    # Actualizar el fotograma previo para el siguiente cálculo\n",
    "    prev_frame = current_frame\n",
    "\n",
    "# Liberar los recursos\n",
    "cv2.destroyAllWindows()\n",
    "video.release()\n",
    "#display_handle.update(None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Metrics Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las listas para almacenar los valores de las métricas\n",
    "AUCj_values = []\n",
    "AUCshuff_values = []\n",
    "NSS_values = []\n",
    "CC_values = []\n",
    "\n",
    "# Definimos los valores de sigma con un intervalo de 25\n",
    "sigmas = [1, 25, 50, 75, 100, 125, 150, 175, 200]\n",
    "\n",
    "# Iteramos sobre los diferentes valores de sigma\n",
    "for sigma in sigmas:\n",
    "    gaussian_AUCjudd_total = 0\n",
    "    gaussian_AUCshuff_total = 0\n",
    "    gaussian_NSS_total = 0\n",
    "    gaussian_CC_total = 0\n",
    "    gaussian_AUCjudd_scores = []\n",
    "    gaussian_AUCshuff_scores = []\n",
    "    gaussian_NSS_scores = []\n",
    "    gaussian_CC_scores = []\n",
    "\n",
    "    # Definimos parámetros y creamos la gaussiana\n",
    "    shape = cv2.imread('./Imaxes/1.jpg', 1).shape\n",
    "    gaussian_sm = CreateGaussian(shape, sigma)\n",
    "\n",
    "    # Recorremos la base de datos con las imágenes iniciales\n",
    "    for i in range(1, nimages+1):\n",
    "        # Leemos la imagen y el mapa de fijación correspondiente\n",
    "        image = cv2.imread(str(images_path)+str(i)+'.jpg', 0)\n",
    "        ground_truth = fixation_maps['white'][0,i-1]*255\n",
    "        random_ground_truth = fixation_maps['white'][0,np.random.randint(0,nimages)]*255\n",
    "\n",
    "        # Calculamos las diferentes metricas\n",
    "        AUCj_score = AUC_judd(gaussian_sm, ground_truth)\n",
    "        gaussian_AUCjudd_scores.append(AUCj_score)\n",
    "        gaussian_AUCjudd_total += AUCj_score\n",
    "\n",
    "        AUCshuff_score = AUC_shuff(gaussian_sm, ground_truth, random_ground_truth)\n",
    "        gaussian_AUCshuff_scores.append(AUCshuff_score)\n",
    "        gaussian_AUCshuff_total += AUCshuff_score\n",
    "\n",
    "        NSS_score = NSS(gaussian_sm, ground_truth)\n",
    "        gaussian_NSS_scores.append(NSS_score)\n",
    "        gaussian_NSS_total += NSS_score\n",
    "\n",
    "        CC_score = CC(gaussian_sm, ground_truth)\n",
    "        gaussian_CC_scores.append(CC_score)\n",
    "        gaussian_CC_total += CC_score\n",
    "\n",
    "    # Calculamos la media de las métricas y las almacenamos en las listas correspondientes\n",
    "    AUCj_values.append(gaussian_AUCjudd_total/nimages)\n",
    "    AUCshuff_values.append(gaussian_AUCshuff_total/nimages)\n",
    "    NSS_values.append(gaussian_NSS_total/nimages)\n",
    "    CC_values.append(gaussian_CC_total/nimages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el DataFrame con las métricas\n",
    "data = {\n",
    "    'Sigma Value':sigmas,\n",
    "    'AUCjudd':AUCj_values,\n",
    "    'AUCshuff':AUCshuff_values,\n",
    "    'NSS':NSS_values,\n",
    "    'CC':CC_values\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Seleccionamos las columnas de interés para el gráfico de líneas\n",
    "x = df['Sigma Value']\n",
    "y1 = df['AUCjudd']\n",
    "y2 = df['AUCshuff']\n",
    "y3 = df['NSS']\n",
    "y4 = df['CC']\n",
    "\n",
    "# Creamos subplots separados para cada métrica\n",
    "fig, axs = plt.subplots(4, 1, figsize=(10, 20))\n",
    "\n",
    "# Graficamos cada métrica en su propio subplot\n",
    "axs[0].plot(x, y1, marker='o', label='AUC judd', color='blue')\n",
    "axs[0].set_title('AUC judd')\n",
    "axs[0].set_xlabel('Valor sigma')\n",
    "axs[0].set_ylabel('Valor métrica')\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].plot(x, y2, marker='o', label='AUC shuffled', color='orange')\n",
    "axs[1].set_title('AUC shuffled')\n",
    "axs[1].set_xlabel('Valor sigma')\n",
    "axs[1].set_ylabel('Valor métrica')\n",
    "axs[1].grid(True)\n",
    "\n",
    "axs[2].plot(x, y3, marker='o', label='NSS', color='green')\n",
    "axs[2].set_title('NSS')\n",
    "axs[2].set_xlabel('Valor sigma')\n",
    "axs[2].set_ylabel('Valor métrica')\n",
    "axs[2].grid(True)\n",
    "\n",
    "axs[3].plot(x, y4, marker='o', label='CC', color='red')\n",
    "axs[3].set_title('CC')\n",
    "axs[3].set_xlabel('Valor sigma')\n",
    "axs[3].set_ylabel('Valor métrica')\n",
    "axs[3].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORKS FINE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SR_Color_Multiescala(img, num_scales=1):\n",
    "    rows, cols = img.shape[0], img.shape[1]\n",
    "\n",
    "    # Si es una imagen en escala de grises (no se puede obtener información de color) \n",
    "    if len(img.shape) == 2:\n",
    "        # Transformada de Fourier\n",
    "        c = cv2.dft(np.float32(img), flags=cv2.DFT_COMPLEX_OUTPUT)\n",
    "        mag = np.sqrt(c[:, :, 0] ** 2 + c[:, :, 1] ** 2)\n",
    "        spectralResidual = np.exp(np.log(mag) - cv2.boxFilter(np.log(mag), -1, (3, 3)))\n",
    "\n",
    "        # Aplicación del residuo espectral en el dominio de la frecuencia\n",
    "        c[:, :, 0] = c[:, :, 0] * spectralResidual / mag\n",
    "        c[:, :, 1] = c[:, :, 1] * spectralResidual / mag\n",
    "        c = cv2.dft(c, flags=(cv2.DFT_INVERSE | cv2.DFT_SCALE))\n",
    "        mag = c[:, :, 0] ** 2 + c[:, :, 1] ** 2\n",
    "\n",
    "        # Suavizado\n",
    "        sm = cv2.GaussianBlur(np.float32(mag),(85,85),20,20)\n",
    "\n",
    "        # Redimensionamos y normalizamos el mapa de saliencia\n",
    "        sm = cv2.normalize(cv2.resize(sm, (cols, rows), cv2.INTER_LANCZOS4), None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "        return sm\n",
    "\n",
    "    # Si es una imagen RGB\n",
    "    else:\n",
    "        # Crear la pirámide gaussiana\n",
    "        pyramid = [img]\n",
    "        for _ in range(num_scales-1):\n",
    "            img = cv2.pyrDown(img)\n",
    "            pyramid.append(img)\n",
    "\n",
    "        # Lista en la que se almacenarán los mapas de saliencia a distintas escalas\n",
    "        smSS = []\n",
    "\n",
    "        for scaled_img in pyramid:\n",
    "            # Obtener los canales B, G, R\n",
    "            B = scaled_img[:, :, 0]\n",
    "            G = scaled_img[:, :, 1]\n",
    "            R = scaled_img[:, :, 2]\n",
    "            channels = [B, G, R]\n",
    "            results = []\n",
    "\n",
    "            # Para cada canal\n",
    "            for i in range(len(channels)):\n",
    "                # Transformada de Fourier para el canal actual\n",
    "                c = cv2.dft(np.float32(channels[i]), flags=cv2.DFT_COMPLEX_OUTPUT)\n",
    "                mag = np.sqrt(c[:, :, 0] ** 2 + c[:, :, 1] ** 2)\n",
    "                spectralResidual = np.exp(np.log(mag) - cv2.boxFilter(np.log(mag), -1, (3, 3)))\n",
    "\n",
    "                # Aplicación del residuo espectral al canal en el dominio de la frecuencia\n",
    "                c[:, :, 0] = c[:, :, 0] * spectralResidual / mag\n",
    "                c[:, :, 1] = c[:, :, 1] * spectralResidual / mag\n",
    "                c = cv2.dft(c, flags=(cv2.DFT_INVERSE | cv2.DFT_SCALE))\n",
    "                mag = c[:, :, 0] ** 2 + c[:, :, 1] ** 2\n",
    "\n",
    "                # Suavizado\n",
    "                sm_channel = cv2.GaussianBlur(np.float32(mag),(85,85),20,20)\n",
    "\n",
    "                # Agregamos el mapa de saliencia del canal actual a la lista\n",
    "                results.append(sm_channel)\n",
    "\n",
    "            # Sumamos los mapas de saliencia de cada canal de color\n",
    "            sm = np.sum(results, axis=0)\n",
    "\n",
    "            # Introducimos el mapa de saliencia para la escala actual en la lista\n",
    "            smSS.append(sm)\n",
    "\n",
    "        # Reconstruir la imagen con las pirámides\n",
    "        sm_ = smSS[0]\n",
    "        for i in range(1, len(smSS)+1):\n",
    "            smSS[-i] = cv2.resize(smSS[-i], (cols, rows))\n",
    "            sm_ = np.add(sm_, smSS[-i])\n",
    "\n",
    "        # Normalización\n",
    "        sm_norm = cv2.normalize(sm_, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "        return sm_norm\n",
    "\n",
    "def PFT_Color_Multiescala(img, num_scales=1):\n",
    "    rows, cols = img.shape[0], img.shape[1]\n",
    "\n",
    "    # Si es una imagen en escala de grises, no podemos obtener información de los colores \n",
    "    if len(img.shape) == 2:\n",
    "        # Transformada de Fourier\n",
    "        img_fft = np.fft.fft2(img)\n",
    "\n",
    "        # Espectro de la fase de la imagen\n",
    "        phase_espectrum = np.angle(img_fft)\n",
    "\n",
    "        # Mapa de saliencia\n",
    "        sm = cv2.GaussianBlur(np.float32((np.abs(np.fft.ifft2(np.exp(1j * phase_espectrum)))**2)),(85,85),20,20)\n",
    "\n",
    "        # Redimensionamos y normalizamos el mapa de saliencia\n",
    "        sm = cv2.normalize(cv2.resize(sm, (cols, rows), cv2.INTER_LANCZOS4), None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "        return sm\n",
    "\n",
    "    # Crear la pirámide gaussiana\n",
    "    pyramid = [img]\n",
    "    for _ in range(num_scales-1):\n",
    "        img = cv2.pyrDown(img)\n",
    "        pyramid.append(img)\n",
    "\n",
    "    # Inicializar el mapa de saliencia final\n",
    "    smSS = []\n",
    "\n",
    "    for scaled_img in pyramid:\n",
    "        # Obtener los canales B, G, R\n",
    "        B = scaled_img[:, :, 0]\n",
    "        G = scaled_img[:, :, 1]\n",
    "        R = scaled_img[:, :, 2]\n",
    "        channels = [B, G, R]\n",
    "        results = []\n",
    "\n",
    "        # Para cada canal\n",
    "        for i in range(len(channels)):\n",
    "            # Transformada de Fourier\n",
    "            img_fft = np.fft.fft2(channels[i])\n",
    "\n",
    "            # Espectro de la fase de la imagen\n",
    "            phase_espectrum = np.angle(img_fft)\n",
    "\n",
    "            # Suavizado\n",
    "            sm_channel = cv2.GaussianBlur(np.float32((np.abs(np.fft.ifft2(np.exp(1j * phase_espectrum)))**2)),(85,85),20,20)\n",
    "\n",
    "            # Agregamos el mapa de saliencia del canal actual a la lista\n",
    "            results.append(sm_channel)\n",
    "\n",
    "        # Sumamos los mapas de saliencia de cada canal de color\n",
    "        sm = np.sum(results, axis=0)\n",
    "\n",
    "        # Introducimos el mapa de saliencia para la escala actual en la lista\n",
    "        smSS.append(sm)\n",
    "\n",
    "    # Reconstruir la imagen con las pirámides\n",
    "    sm_ = smSS[0]\n",
    "    for i in range(1, len(smSS)+1):\n",
    "        smSS[-i] = cv2.resize(smSS[-i], (cols, rows))\n",
    "        sm_ = np.add(sm_, smSS[-i])\n",
    "\n",
    "    # Normalización\n",
    "    sm_norm = cv2.normalize(cv2.resize(sm_, (cols, rows), cv2.INTER_LANCZOS4), None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "    return sm_norm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DINÁMICA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD PFT_VIDEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular el mapa de saliencia basado en el espectro de fase de la transformada de Fourier para video\n",
    "def PFT_video(prev_frame, current_frame):\n",
    "    # Si es una imagen en escala de grises, no podemos obtener información de los colores \n",
    "    if len(current_frame.shape) == 2:\n",
    "        print(\"Imagen en escala de grises, devolviendo la imagen original...\")\n",
    "        return\n",
    "\n",
    "    rows, cols = current_frame.shape[0], current_frame.shape[1]\n",
    "\n",
    "    b, g, r = cv2.split(current_frame)\n",
    "\n",
    "    # Canal de color\n",
    "    R = r - (g + b)/2\n",
    "    G = g - (r + b)/2\n",
    "    B = b - (r + g)/2\n",
    "    Y = (r + g)/2 - np.abs(r - g)/2 - b\n",
    "\n",
    "    # Canales componente-opuesta de color\n",
    "    RG = R - G\n",
    "    BY = B - Y\n",
    "\n",
    "    # Canal de intensidad\n",
    "    I = np.sum(current_frame, axis=-1)\n",
    "\n",
    "    # Canal de motion\n",
    "    prev_I = np.sum(prev_frame, axis=-1)\n",
    "    motion = np.abs(I-prev_I)\n",
    "\n",
    "    # Representación de q(t) en forma simplectica\n",
    "    # f1(t):\n",
    "    planes = [motion.astype(np.float64), RG.astype(np.float64)]\n",
    "    f1 = cv2.dft(cv2.merge(planes))\n",
    "    planes = cv2.split(f1)\n",
    "\n",
    "    # Magnitud del espectro1\n",
    "    mag1 = cv2.magnitude(planes[0], planes[1])\n",
    "    mag1 = cv2.multiply(mag1, mag1)\n",
    "\n",
    "    # f2(t):\n",
    "    planes = [BY.astype(np.float64), I.astype(np.float64)]\n",
    "    f2 = cv2.dft(cv2.merge(planes))\n",
    "    planes = cv2.split(f2)\n",
    "\n",
    "    # Magnitud del espectro2\n",
    "    mag2 = cv2.magnitude(planes[0], planes[1])\n",
    "    mag2 = cv2.multiply(mag2, mag2)\n",
    "\n",
    "    # Combinamos las magnitudes\n",
    "    mag = cv2.sqrt(mag1 + mag2)\n",
    "\n",
    "    # Normalizar\n",
    "    # f1(t):\n",
    "    planes = list(cv2.split(f1))\n",
    "    planes[0] = planes[0]/mag\n",
    "    planes[1] = planes[1]/mag\n",
    "    f1 = cv2.merge(planes)\n",
    "\n",
    "    # f2(t):\n",
    "    planes = list(cv2.split(f2))\n",
    "    planes[0] = planes[0]/mag\n",
    "    planes[1] = planes[1]/mag\n",
    "    f2 = cv2.merge(planes)\n",
    "\n",
    "    # Transformada inversa de Fourier\n",
    "    cv2.dft(f1, f1, cv2.DFT_INVERSE)\n",
    "    cv2.dft(f2, f2, cv2.DFT_INVERSE)\n",
    "\n",
    "    # Magnitud de la tranformada inversa de Fourier\n",
    "    planes = cv2.split(f1)\n",
    "    mag1 = cv2.magnitude(planes[0], planes[1])\n",
    "    mag1 = cv2.multiply(mag1, mag1)\n",
    "\n",
    "    planes = cv2.split(f2)\n",
    "    mag2 = cv2.magnitude(planes[0], planes[1])\n",
    "    mag2 = cv2.multiply(mag2, mag2)\n",
    "\n",
    "    # Magnitud total\n",
    "    mag = cv2.sqrt(mag1 + mag2)\n",
    "\n",
    "    # Suavizado de la imagen\n",
    "    mag = cv2.GaussianBlur(mag, (5, 5), 8, None, 8)\n",
    "\n",
    "    # Normalizar el mapa de saliencia\n",
    "    sm = cv2.normalize(mag, np.zeros((rows, cols, 1), np.uint8), 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "\n",
    "    return sm\n",
    "\n",
    "\n",
    "# Cargamos los videos\n",
    "videos_list = os.listdir(\"./videos\")\n",
    "for i in range(len(videos_list)):\n",
    "    cap = cv2.VideoCapture(\"./videos/\"+videos_list[i])\n",
    "\n",
    "    # Leer el primer fotograma\n",
    "    ret, prev_frame = cap.read()\n",
    "\n",
    "    # Iterar sobre los fotogramas del video\n",
    "    while cap.isOpened():\n",
    "        ret, current_frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Llamamos a la función para calcular el mapa de saliencia a multiescala con información de color e invariante a la escala\n",
    "        sm = PFT_video(prev_frame, current_frame)\n",
    "\n",
    "        # Visualización\n",
    "        cv2.imshow('Saliency map {}'.format(videos_list[i]), sm)\n",
    "\n",
    "        # Actualizamos\n",
    "        prev_frame = current_frame\n",
    "\n",
    "        if cv2.waitKey(15) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Liberar recursos\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD PDZHOU+COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular el mapa de saliencia mediante la discrepancia de la fase\n",
    "def PDZhou_color_SM(prev_frame, current_frame):\n",
    "    # Obtenemos la dimensión de los frames\n",
    "    shape_frame = (prev_frame.shape[1], prev_frame.shape[0])    # (width, height)\n",
    "\n",
    "    # Mapa de calor por colores:\n",
    "    # Convertir el fotograma a espacio de color LAB\n",
    "    lab_frame = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # Separar los canales LAB\n",
    "    l, a, b = cv2.split(lab_frame)\n",
    "\n",
    "    # Normalizar los canales a valores entre 0 y 1\n",
    "    l_normalized = cv2.normalize(l, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "    a_normalized = cv2.normalize(a, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "    b_normalized = cv2.normalize(b, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "    # Calcular la saliencia basada en todos los canales LAB\n",
    "    sm_c = cv2.GaussianBlur(l_normalized, (5, 5), sigmaX=8, sigmaY=8)\n",
    "    sm_c += cv2.GaussianBlur(a_normalized, (5, 5), sigmaX=8, sigmaY=8)\n",
    "    sm_c += cv2.GaussianBlur(b_normalized, (5, 5), sigmaX=8, sigmaY=8)\n",
    "\n",
    "    # Escalar a valores entre 0 y 255 y luego invertir los colores\n",
    "    sm_c = 255 - (sm_c * 255 / np.max(sm_c))\n",
    "\n",
    "    # Mapa de calor por movimiento:\n",
    "    # Escalar y redimensionar los fotogramas de entrada\n",
    "    prev_frame = cv2.resize(cv2.normalize(prev_frame, None, 0.0, 1.0, cv2.NORM_MINMAX, dtype=cv2.CV_32F), (320, 240))\n",
    "    current_frame = cv2.resize(cv2.normalize(current_frame, None, 0.0, 1.0, cv2.NORM_MINMAX, dtype=cv2.CV_32F), (320, 240))\n",
    "\n",
    "    # Calcular la transformada de Fourier de los fotogramas\n",
    "    prev_FFT = np.fft.fft2(prev_frame)\n",
    "    current_FFT = np.fft.fft2(current_frame)\n",
    "\n",
    "    # Calcular la magnitud y fase de las transformadas de Fourier\n",
    "    prev_mag = np.abs(prev_FFT)\n",
    "    current_mag = np.abs(current_FFT)\n",
    "    prev_phase = np.angle(prev_FFT)\n",
    "    current_phase = np.angle(current_FFT)\n",
    "\n",
    "    # Calcular los mapas de magnitud usando la discrepancia de fase\n",
    "    mMap1 = np.abs(np.fft.ifft2((current_mag - prev_mag) * np.exp(1j * prev_phase)))\n",
    "    mMap2 = np.abs(np.fft.ifft2((current_mag - prev_mag) * np.exp(1j * current_phase)))\n",
    "\n",
    "    # Multiplicar los mapas de magnitud para obtener la imagen de saliencia\n",
    "    img = mMap1 * mMap2\n",
    "\n",
    "    # Sumar sobre los canales de color para obtener un solo mapa de saliencia\n",
    "    sm_m = np.sum(img, axis=2)\n",
    "\n",
    "    # Media de ambos mapas de calor:\n",
    "    sm = (sm_c*0.01 + sm_m*100) / 2 # Le quitamos peso a la información de color y se lo damos al movimiento para nivelarlos\n",
    "\n",
    "    # Redimensionar el mapa de saliencia a la forma deseada\n",
    "    sm_resized = cv2.resize(sm, shape_frame)\n",
    "    \n",
    "    # Normalizar el mapa de saliencia para el rango 0-255\n",
    "    sm_norm = cv2.normalize(sm_resized, None, 0.0, 255, cv2.NORM_MINMAX, cv2.CV_32F)\n",
    "\n",
    "    return sm_norm\n",
    "\n",
    "\n",
    "# Leer el video\n",
    "video = cv2.VideoCapture('./videos/SCE_bur_1_distractores_altos.avi')\n",
    "\n",
    "# Leer el primer fotograma\n",
    "ret, prev_frame = video.read()\n",
    "display_handle = display(None, display_id=True)\n",
    "while True:\n",
    "    ret, current_frame = video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Calcular la saliencia\n",
    "    sm = PDZhou_color_SM(prev_frame, current_frame)\n",
    "\n",
    "    # Visualización del mapa saliencia\n",
    "    ret, sm = cv2.imencode('.jpg', sm)\n",
    "    display_handle.update(Image(data=sm.tobytes()))\n",
    "    cv2.waitKey(10)\n",
    "\n",
    "    # Actualizar el fotograma previo para el siguiente cálculo\n",
    "    prev_frame = current_frame\n",
    "\n",
    "# Liberar los recursos\n",
    "video.release()\n",
    "display_handle.update(None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PFT (VIDEO) INCORRECTO (2 FRAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 64\n",
    "\n",
    "# Función para calcular el mapa de saliencia basado en el espectro de fase de la transformada de Fourier\n",
    "def PFTVideo_Color_Multiescala(previous_frame, current_frame, num_scales=1):\n",
    "    if len(previous_frame.shape) == 2:\n",
    "        rows, cols = previous_frame.shape\n",
    "    else:\n",
    "        rows, cols, _ = previous_frame.shape\n",
    "\n",
    "    previous_frame = cv2.resize(previous_frame, (WIDTH, int(WIDTH * previous_frame.shape[0] / previous_frame.shape[1])))\n",
    "    current_frame = cv2.resize(current_frame, (WIDTH, int(WIDTH * current_frame.shape[0] / current_frame.shape[1])))\n",
    "\n",
    "    # Crear la pirámide gaussiana para previous_frame\n",
    "    previous_pyramid = [previous_frame]\n",
    "    for _ in range(num_scales - 1):\n",
    "        previous_frame = cv2.pyrDown(previous_frame)\n",
    "        previous_pyramid.append(previous_frame)\n",
    "\n",
    "    # Crear la pirámide gaussiana para current_frame\n",
    "    current_pyramid = [current_frame]\n",
    "    for _ in range(num_scales - 1):\n",
    "        current_frame = cv2.pyrDown(current_frame)\n",
    "        current_pyramid.append(current_frame)\n",
    "\n",
    "    # Lista en la que se almacenarán los mapas de saliencia a distintas escalas\n",
    "    smSS = []\n",
    "\n",
    "    for prev, curr in zip(previous_pyramid, current_pyramid):\n",
    "        # Si son imágenes en escala de grises, no podemos obtener información de los colores\n",
    "        if len(prev.shape) == 2:\n",
    "            # Calculamos la diferencia entre los frames\n",
    "            diff = cv2.absdiff(prev, curr)\n",
    "\n",
    "            # Transformada de Fourier\n",
    "            diff_fft = np.fft.fft2(diff)\n",
    "\n",
    "            # Espectro de la fase de la imagen\n",
    "            phase_spectrum = np.angle(diff_fft)\n",
    "\n",
    "            # Suavizado y redimensionado del mapa de saliencia\n",
    "            sm = cv2.resize(cv2.GaussianBlur((np.abs(np.fft.ifft2(np.exp(1j * phase_spectrum))) ** 2),(11,11),3,3),(cols, rows), cv2.NORM_MINMAX)\n",
    "\n",
    "            # Introducimos el mapa de saliencia para la escala actual en la lista\n",
    "            smSS.append(sm)\n",
    "\n",
    "        # Si son imágenes a color\n",
    "        else:\n",
    "            # Obtener los canales B, G, R\n",
    "            B_prev, G_prev, R_prev = prev[:, :, 0], prev[:, :, 1], prev[:, :, 2]\n",
    "            B_curr, G_curr, R_curr = curr[:, :, 0], curr[:, :, 1], curr[:, :, 2]\n",
    "\n",
    "            # Calculamos la diferencia entre los frames para cada canal\n",
    "            diff_B = cv2.absdiff(B_prev, B_curr)\n",
    "            diff_G = cv2.absdiff(G_prev, G_curr)\n",
    "            diff_R = cv2.absdiff(R_prev, R_curr)\n",
    "\n",
    "            channels_diff = [diff_B, diff_G, diff_R]\n",
    "            results = []\n",
    "\n",
    "            # Para cada canal de diferencia\n",
    "            for i in range(len(channels_diff)):\n",
    "                # Transformada de Fourier\n",
    "                diff_fft = np.fft.fft2(channels_diff[i])\n",
    "\n",
    "                # Espectro de la fase de la imagen\n",
    "                phase_spectrum = np.angle(diff_fft)\n",
    "\n",
    "                # Suavizado y redimensionado del mapa de saliencia para el canal actual\n",
    "                sm_channel = cv2.resize(cv2.GaussianBlur((np.abs(np.fft.ifft2(np.exp(1j * phase_spectrum))) ** 2),(11,11),3,3), (cols, rows), cv2.NORM_MINMAX)\n",
    "\n",
    "                # Agregamos el mapa de saliencia del canal actual a la lista\n",
    "                results.append(sm_channel)\n",
    "\n",
    "            # Sumamos los mapas de saliencia de cada canal de color\n",
    "            sm = np.sum(results, axis=0)\n",
    "\n",
    "            # Introducimos el mapa de saliencia para la escala actual en la lista\n",
    "            smSS.append(sm)\n",
    "\n",
    "    # Reconstruir la imagen con las pirámides\n",
    "    sm_ = smSS[0]\n",
    "    for i in range(1, len(smSS) + 1):\n",
    "        smSS[-i] = cv2.resize(smSS[-i], (cols, rows), cv2.NORM_MINMAX)\n",
    "        sm_ = np.add(sm_, smSS[-i]) \n",
    "\n",
    "    # Normalización\n",
    "    sm_norm = cv2.normalize(sm_, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "    return sm_norm\n",
    "\n",
    "\n",
    "# Cargamos los videos\n",
    "videos_list = os.listdir(\"./videos\")\n",
    "for i in range(len(videos_list)):\n",
    "    video = cv2.VideoCapture(\"./videos/\"+videos_list[i])\n",
    "\n",
    "    # Leer el primer fotograma\n",
    "    ret, prev_frame = video.read()\n",
    "\n",
    "    # Iterar sobre los fotogramas del video\n",
    "    while video.isOpened():\n",
    "        ret, current_frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Llamamos a la función para calcular el mapa de saliencia a multiescala con información de color e invariante a la escala\n",
    "        sm = PFTVideo_Color_Multiescala(prev_frame, current_frame, num_scales=1)\n",
    "\n",
    "        # Visualización\n",
    "        cv2.imshow('Saliency map {}'.format(videos_list[i]), sm)\n",
    "\n",
    "        # Actualizamos\n",
    "        prev_frame = current_frame\n",
    "\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Liberar recursos\n",
    "    video.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_AUC_scores = []\n",
    "gp_NSS_scores = []\n",
    "gp_CC_scores = []\n",
    "pd_AUCjudd_scores = []\n",
    "pd_AUCshuff_scores = []\n",
    "pd_NSS_scores = []\n",
    "pd_CC_scores = []\n",
    "\n",
    "\n",
    "# Recorremos la base de datos con los videos iniciales\n",
    "for ivideo in range(fixation_maps['EyeTrackVDB'].shape[0]):\n",
    "    print(ivideo)\n",
    "    # Leemos el video y el mapa de fijación correspondiente\n",
    "    name_video = fixation_maps['EyeTrackVDB'][ivideo]['Name'][0][0]\n",
    "    video = cv2.VideoCapture(videos_path + name_video)\n",
    "\n",
    "    if (video.isOpened() == False):\n",
    "        print(\"Error en la apertura del fichero {}\".format(videos_path + name_video))\n",
    "\n",
    "    # Recuperamos as propieades do video.\n",
    "    frame_w = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_h = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    frame_count = 0\n",
    "    ret, prev_frame = video.read()\n",
    "    while(True):\n",
    "        ret, current_frame = video.read()\n",
    "        if(not ret):\n",
    "            break\n",
    "\n",
    "        # Obtenemos el mapa de saliencia para el frame\n",
    "        #gp_sm_frame = PFTVideo_Color_Multiescala(prev_frame, current_frame)  # Fase global\n",
    "        pd_sm_frame = PDZhou(prev_frame, current_frame)  # Discrepancia de la fase\n",
    "\n",
    "        # Leer las fijaciones del frame\n",
    "        fix = fixation_maps['EyeTrackVDB'][ivideo]['Frame'][0]['FixL'][0,frame_count][:,0:2]\n",
    "        other_fix = fixation_maps['EyeTrackVDB'][ivideo]['Frame'][0]['FixL'][0,np.random.randint(len(fixation_maps['EyeTrackVDB'][ivideo]['Frame'][0]['FixL'][0]))][:,0:2]\n",
    "\n",
    "        # Construir el mapa de densidad de fijaciones\n",
    "        map_fix_frame = np.zeros((frame_h,frame_w))\n",
    "        for ifix in range(fix.shape[0]):\n",
    "            # Puede haber fijaciones fuera de los margenes de la imagen\n",
    "            if(int(fix[ifix][0]) < frame_h and int(fix[ifix][1]) < frame_w):\n",
    "                map_fix_frame[int(fix[ifix][1]), int(fix[ifix][0])] = 255   # Mapa de fijaciones\n",
    "        # Convolucionamos con una gaussiana las fijaciones para obtener el mapa de densidad de fijaciones\n",
    "        map_den_fix = (np.floor(cv2.normalize(cv2.GaussianBlur(np.float32(map_fix_frame),(85,85),20,20), None, 0, 255, cv2.NORM_MINMAX))).astype(np.uint8)\n",
    "\n",
    "        # Construir otro mapa de densidad de fijaciones\n",
    "        other_map_fix_frame = np.zeros((frame_h,frame_w))\n",
    "        for ifix in range(other_fix.shape[0]):\n",
    "            # Puede haber fijaciones fuera de los margenes de la imagen\n",
    "            if(int(other_fix[ifix][0]) < frame_h and int(other_fix[ifix][1]) < frame_w):\n",
    "                other_map_fix_frame[int(other_fix[ifix][1]), int(other_fix[ifix][0])] = 255 # Mapa de fijaciones\n",
    "        # Convolucionamos con una gaussiana las fijaciones para obtener el mapa de densidad de fijaciones\n",
    "        other_map_den_fix = (np.floor(cv2.normalize(cv2.GaussianBlur(np.float32(other_map_fix_frame),(85,85),20,20), None, 0, 255, cv2.NORM_MINMAX))).astype(np.uint8)\n",
    "\n",
    "\n",
    "        # Calculamos las diferentes métricas para cada frame\n",
    "        # Fase global:\n",
    "        \"\"\"gp_AUC_score_frame = AUC_shuff(gp_sm_frame, map_den_fix, other_map_den_fix)\n",
    "        gp_AUC_scores.append(gp_AUC_score_frame)\n",
    "\n",
    "        gp_NSS_score_frame = NSS(gp_sm_frame, map_den_fix)\n",
    "        gp_NSS_scores.append(gp_NSS_score_frame)\n",
    "\n",
    "        gp_CC_score_frame = CC(gp_sm_frame, map_den_fix)\n",
    "        gp_CC_scores.append(gp_CC_score_frame)\"\"\"\n",
    "\n",
    "        # Discrepancia de la fase:\n",
    "        pd_AUCjudd_scores.append(AUC_judd(pd_sm_frame, map_den_fix))\n",
    "\n",
    "        pd_AUCshuff_scores.append(AUC_shuff(pd_sm_frame, map_den_fix, other_map_den_fix))\n",
    "\n",
    "        pd_NSS_scores.append(NSS(pd_sm_frame, map_den_fix))\n",
    "\n",
    "        pd_CC_scores.append(CC(pd_sm_frame, map_den_fix))\n",
    "\n",
    "        # Actualizamos los frames\n",
    "        frame_count += 1\n",
    "        prev_frame = current_frame\n",
    "\n",
    "# Calculamos la media de la métricas\n",
    "\"\"\"gp_AUC_mean = np.mean(np.nan_to_num(gp_AUC_scores))\n",
    "gp_NSS_mean = np.mean(np.nan_to_num(gp_NSS_scores))\n",
    "gp_CC_mean = np.mean(np.nan_to_num(gp_CC_scores))\"\"\"\n",
    "pd_AUCjudd_mean = np.mean(np.nan_to_num(pd_AUCjudd_scores))\n",
    "pd_AUCshuff_mean = np.mean(np.nan_to_num(pd_AUCshuff_scores))\n",
    "pd_NSS_mean = np.mean(np.nan_to_num(pd_NSS_scores))\n",
    "pd_CC_mean = np.mean(np.nan_to_num(pd_CC_scores))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/.virtualenvs/VAA/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/adrian/.virtualenvs/VAA/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/adrian/.virtualenvs/VAA/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/adrian/.virtualenvs/VAA/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n",
      "59\n",
      "86\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl Kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. Revise el código de las celdas para identificar una posible causa del error. Haga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. Vea el [registro] de Jupyter (command:jupyter.viewOutput) para obtener más detalles."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from IPython.display import display, Image\n",
    "\n",
    "def PDZhou(prev_frame, current_frame):\n",
    "    # Si el video está en escala de grises\n",
    "    if len(prev_frame) == 2:\n",
    "        # Calculamos la Transformada de Fourier 2D para cada fotograma\n",
    "        FFT1 = np.fft.fft2(prev_frame)\n",
    "        FFT2 = np.fft.fft2(current_frame)\n",
    "\n",
    "        # Calculamos la magnitud y la fase de cada transformada de Fourier\n",
    "        Amp1 = np.abs(FFT1)\n",
    "        Amp2 = np.abs(FFT2)\n",
    "        Phase1 = np.angle(FFT1)\n",
    "        Phase2 = np.angle(FFT2)\n",
    "\n",
    "        # Calculamos los mapas de magnitud\n",
    "        mMap1 = np.abs(np.fft.ifft2((Amp2 - Amp1) * np.exp(1j * Phase1)))\n",
    "        mMap2 = np.abs(np.fft.ifft2((Amp2 - Amp1) * np.exp(1j * Phase2)))\n",
    "\n",
    "        # Calculamos el mapa combinado\n",
    "        mMap = mMap1 * mMap2\n",
    "\n",
    "        # Suavizado\n",
    "        mMap = cv2.GaussianBlur(mMap,(85,85),20,20)\n",
    "\n",
    "    # Si tiene canales de color\n",
    "    else:\n",
    "        # Obtenemos los canales de color\n",
    "        prev_frame_channels = [prev_frame[:, :, 0], prev_frame[:, :, 1], prev_frame[:, :, 2]]\n",
    "        current_frame_channels = [current_frame[:, :, 0], current_frame[:, :, 1], current_frame[:, :, 2]]\n",
    "        results = []\n",
    "\n",
    "        # Calculamos el mapa de saliencia para cada canal de color\n",
    "        for i in range(len(prev_frame_channels)):\n",
    "            # Calculamos la Transformada de Fourier 2D para cada fotograma\n",
    "            FFT1 = np.fft.fft2(prev_frame_channels[i])\n",
    "            FFT2 = np.fft.fft2(current_frame_channels[i])\n",
    "\n",
    "            # Calculamos la magnitud y la fase de cada transformada de Fourier\n",
    "            Amp1 = np.abs(FFT1)\n",
    "            Amp2 = np.abs(FFT2)\n",
    "            Phase1 = np.angle(FFT1)\n",
    "            Phase2 = np.angle(FFT2)\n",
    "\n",
    "            # Calculamos los mapas de magnitud\n",
    "            mMap1 = np.abs(np.fft.ifft2((Amp2 - Amp1) * np.exp(1j * Phase1)))\n",
    "            mMap2 = np.abs(np.fft.ifft2((Amp2 - Amp1) * np.exp(1j * Phase2)))\n",
    "\n",
    "            # Calculamos el mapa combinado\n",
    "            mMap_channel = mMap1 * mMap2\n",
    "\n",
    "            # Suavizado\n",
    "            mMap_channel = cv2.GaussianBlur(mMap_channel,(85,85),20,20)\n",
    "\n",
    "            # Agregamos el mapa de saliencia del canal actual a la lista\n",
    "            results.append(mMap_channel)\n",
    "\n",
    "        # Sumamos los mapas de saliencia de cada canal de color\n",
    "        mMap = np.sum(results, axis=0)\n",
    "\n",
    "    # Normalización\n",
    "    mMap_norm = cv2.normalize(mMap, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "    return mMap_norm\n",
    "\n",
    "\n",
    "\n",
    "def normalize_map(s_map):\n",
    "\t# normalize the salience map\n",
    "\tnorm_s_map = (s_map - np.min(s_map))/((np.max(s_map)-np.min(s_map))*1.0)\n",
    "\treturn norm_s_map\n",
    "\n",
    "def discretize_gt(gt):\n",
    "\treturn gt/255\n",
    "\n",
    "def AUC_judd(s_map,gt):\n",
    "\t# ground truth is discrete, s_map is continous and normalized\n",
    "\tgt = discretize_gt(gt)\n",
    "\ts_map = normalize_map(s_map)\n",
    "\t# thresholds are calculated from the salience map, only at places where fixations are present\n",
    "\tthresholds = []\n",
    "\tfor i in range(0,gt.shape[0]):\n",
    "\t\tfor k in range(0,gt.shape[1]):\n",
    "\t\t\tif gt[i][k]>0:\n",
    "\t\t\t\tthresholds.append(s_map[i][k])\n",
    "\n",
    "\tnum_fixations = np.sum(gt)\n",
    "\t# num fixations is no. of salience map values at gt >0\n",
    "\tthresholds = sorted(set(thresholds))\n",
    "\t#fp_list = []\n",
    "\t#tp_list = []\n",
    "\tarea = []\n",
    "\tarea.append((0.0,0.0))\n",
    "\tfor thresh in thresholds:\n",
    "\t\t# in the salience map, keep only those pixels with values above threshold\n",
    "\t\ttemp = np.zeros(s_map.shape)\n",
    "\t\ttemp[s_map>=thresh] = 1.0\n",
    "\t\tnum_overlap = np.where(np.add(temp,gt)==2)[0].shape[0]\n",
    "\t\ttp = num_overlap/(num_fixations*1.0)\n",
    "\t\t\n",
    "\t\t# total number of pixels > threshold - number of pixels that overlap with gt / total number of non fixated pixels\n",
    "\t\t# this becomes nan when gt is full of fixations..this won't happen\n",
    "\t\tfp = (np.sum(temp) - num_overlap)/((np.shape(gt)[0] * np.shape(gt)[1]) - num_fixations)\n",
    "\t\t\n",
    "\t\tarea.append((round(tp,4),round(fp,4)))\n",
    "\t\t#tp_list.append(tp)\n",
    "\t\t#fp_list.append(fp)\n",
    "\n",
    "\t#tp_list.reverse()\n",
    "\t#fp_list.reverse()\n",
    "\tarea.append((1.0,1.0))\n",
    "\t#tp_list.append(1.0)\n",
    "\t#fp_list.append(1.0)\n",
    "\t#print tp_list\n",
    "\tarea.sort(key = lambda x:x[0])\n",
    "\ttp_list =  [x[0] for x in area]\n",
    "\tfp_list =  [x[1] for x in area]\n",
    "\treturn np.trapz(np.array(tp_list),np.array(fp_list))\n",
    "\n",
    "def AUC_shuff(s_map,gt,other_map,splits=100,stepsize=0.1):\n",
    "\tgt = discretize_gt(gt)\n",
    "\tother_map = discretize_gt(other_map)\n",
    "\ts_map = normalize_map(s_map)\n",
    "\n",
    "\tnum_fixations = np.sum(gt)\n",
    "\t\n",
    "\tx,y = np.where(other_map==1)\n",
    "\tother_map_fixs = []\n",
    "\tfor j in zip(x,y):\n",
    "\t\tother_map_fixs.append(j[0]*other_map.shape[0] + j[1])\n",
    "\tind = len(other_map_fixs)\n",
    "\tassert ind == len(other_map_fixs), 'something is wrong in auc shuffle'\n",
    "\n",
    "\n",
    "\tnum_fixations_other = min(ind,num_fixations)\n",
    "\n",
    "\tnum_pixels = s_map.shape[0]*s_map.shape[1]\n",
    "\trandom_numbers = []\n",
    "\tfor i in range(0,splits):\n",
    "\t\ttemp_list = []\n",
    "\t\tt1 = np.random.permutation(ind)\n",
    "\t\tfor k in t1:\n",
    "\t\t\ttemp_list.append(other_map_fixs[k])\n",
    "\t\trandom_numbers.append(temp_list)\t\n",
    "\n",
    "\taucs = []\n",
    "\t# for each split, calculate auc\n",
    "\tfor i in random_numbers:\n",
    "\t\tr_sal_map = []\n",
    "\t\tfor k in i:\n",
    "\t\t\t#print('{}  {}'.format(k%s_map.shape[0]-1, int(np.floor(k/s_map.shape[0]))))\n",
    "\t\t\tr_sal_map.append(s_map[k%s_map.shape[0]-1, int(np.floor(k/s_map.shape[0]))])\n",
    "\t\t# in these values, we need to find thresholds and calculate auc\n",
    "\t\tthresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "\t\tr_sal_map = np.array(r_sal_map)\n",
    "\n",
    "\t\t# once threshs are got\n",
    "\t\tthresholds = sorted(set(thresholds))\n",
    "\t\tarea = []\n",
    "\t\tarea.append((0.0,0.0))\n",
    "\t\tfor thresh in thresholds:\n",
    "\t\t\t# in the salience map, keep only those pixels with values above threshold\n",
    "\t\t\ttemp = np.zeros(s_map.shape)\n",
    "\t\t\ttemp[s_map>=thresh] = 1.0\n",
    "\t\t\tnum_overlap = np.where(np.add(temp,gt)==2)[0].shape[0]\n",
    "\t\t\ttp = num_overlap/(num_fixations*1.0)\n",
    "\t\t\t\n",
    "\t\t\t#fp = (np.sum(temp) - num_overlap)/((np.shape(gt)[0] * np.shape(gt)[1]) - num_fixations)\n",
    "\t\t\t# number of values in r_sal_map, above the threshold, divided by num of random locations = num of fixations\n",
    "\t\t\tfp = len(np.where(r_sal_map>thresh)[0])/(num_fixations*1.0)\n",
    "\n",
    "\t\t\tarea.append((round(tp,4),round(fp,4)))\n",
    "\t\t\n",
    "\t\tarea.append((1.0,1.0))\n",
    "\t\tarea.sort(key = lambda x:x[0])\n",
    "\t\ttp_list =  [x[0] for x in area]\n",
    "\t\tfp_list =  [x[1] for x in area]\n",
    "\n",
    "\t\taucs.append(np.trapz(np.array(tp_list),np.array(fp_list)))\n",
    "\t\n",
    "\treturn np.mean(aucs)\n",
    "\n",
    "def NSS(s_map,gt):\n",
    "\tgt = discretize_gt(gt)\n",
    "\ts_map_norm = (s_map - np.mean(s_map))/np.std(s_map)\n",
    "\n",
    "\tx,y = np.where(gt==1)\n",
    "\ttemp = []\n",
    "\tfor i in zip(x,y):\n",
    "\t\ttemp.append(s_map_norm[i[0],i[1]])\n",
    "\treturn np.mean(temp)\n",
    "\n",
    "\n",
    "def CC(s_map,gt):\n",
    "\ts_map_norm = (s_map - np.mean(s_map))/np.std(s_map)\n",
    "\tgt_norm = (gt - np.mean(gt))/np.std(gt)\n",
    "\ta = s_map_norm\n",
    "\tb= gt_norm\n",
    "\tr = (a*b).sum() / np.sqrt((a*a).sum() * (b*b).sum())\n",
    "\treturn r\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Definimos el path a los videos\n",
    "videos_path = './videos/'\n",
    "\n",
    "# Cargamos los mapas de fijación\n",
    "fixation_maps =  sio.loadmat('./FixVideoData.mat')\n",
    "\n",
    "# Definimos el número de videos a procesar\n",
    "nvideos = 6\n",
    "\n",
    "# Recorremos la base de datos con los videos iniciales\n",
    "for ivideo in range(fixation_maps['EyeTrackVDB'].shape[0]):\n",
    "    # Leemos el video y el mapa de fijación correspondiente\n",
    "    name_video = fixation_maps['EyeTrackVDB'][ivideo]['Name'][0][0]\n",
    "    video = cv2.VideoCapture(videos_path + name_video)\n",
    "\n",
    "    if (video.isOpened() == False):\n",
    "        print(\"Error en la apertura del fichero {}\".format(videos_path + name_video))\n",
    "\n",
    "    # Recuperamos as propieades do video.\n",
    "    frame_w = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_h = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    frame_count = 0\n",
    "    ret, prev_frame = video.read()\n",
    "    while(True):\n",
    "        ret, current_frame = video.read()\n",
    "        if(not ret):\n",
    "            break\n",
    "\n",
    "        # Obtenemos el mapa de saliencia para el frame\n",
    "        #gp_sm_frame = PFTVideo_Color_Multiescala(prev_frame, current_frame)  # Fase global\n",
    "        pd_sm_frame = PDZhou(prev_frame, current_frame)  # Discrepancia de la fase\n",
    "\n",
    "        # Leer las fijaciones del frame\n",
    "        fix = fixation_maps['EyeTrackVDB'][ivideo]['Frame'][0]['FixL'][0,frame_count][:,0:2]\n",
    "        randNum = np.random.randint(len(fixation_maps['EyeTrackVDB'][ivideo]['Frame'][0]['FixL'][0]))\n",
    "        print(randNum)\n",
    "        other_fix = fixation_maps['EyeTrackVDB'][ivideo]['Frame'][0]['FixL'][0,randNum][:,0:2]\n",
    "\n",
    "        # Construir el mapa de densidad de fijaciones\n",
    "        map_fix_frame = np.zeros((frame_h,frame_w))\n",
    "        for ifix in range(fix.shape[0]):\n",
    "            # Puede haber fijaciones fuera de los margenes de la imagen\n",
    "            if(int(fix[ifix][0]) < frame_h and int(fix[ifix][1]) < frame_w):\n",
    "                map_fix_frame[int(fix[ifix][1]), int(fix[ifix][0])] = 255   # Mapa de fijaciones\n",
    "        # Convolucionamos con una gaussiana las fijaciones para obtener el mapa de densidad de fijaciones\n",
    "        map_den_fix = (np.floor(cv2.normalize(cv2.GaussianBlur(np.float32(map_fix_frame),(85,85),20,20), None, 0, 255, cv2.NORM_MINMAX))).astype(np.uint8)\n",
    "\n",
    "        # Visualización\n",
    "        cv2.imshow('Saliency map {}'.format(randNum), map_den_fix)\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Actualizamos\n",
    "        prev_frame = current_frame\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Liberar recursos\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n",
      "160.0\n",
      "302\n",
      "302.0\n",
      "67\n",
      "67.0\n",
      "129\n",
      "129.0\n",
      "402\n",
      "402.0\n",
      "302\n",
      "302.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'if (video.isOpened() == False):\\n    print(\"Error en la apertura del fichero {}\".format(videos_path + name_video))\\n\\n# Recuperamos as propieades do video.\\nframe_w = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\\nframe_h = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\\n\\nframe_count = 0\\nret, prev_frame = video.read()\\nwhile(True):\\n    ret, current_frame = video.read()\\n    if(not ret):\\n        break\\n\\n    # Obtenemos el mapa de saliencia para el frame\\n    #gp_sm_frame = PFTVideo_Color_Multiescala(prev_frame, current_frame)  # Fase global\\n    pd_sm_frame = PDZhou(prev_frame, current_frame)  # Discrepancia de la fase\\n\\n    # Leer las fijaciones del frame\\n    fix = fixation_maps[\\'EyeTrackVDB\\'][ivideo][\\'Frame\\'][0][\\'FixL\\'][0,frame_count][:,0:2]\\n    randNum = np.random.randint(len(fixation_maps[\\'EyeTrackVDB\\'][ivideo][\\'Frame\\'][0][\\'FixL\\'][0]))\\n    print(randNum)\\n    other_fix = fixation_maps[\\'EyeTrackVDB\\'][ivideo][\\'Frame\\'][0][\\'FixL\\'][0,randNum][:,0:2]\\n\\n    # Construir el mapa de densidad de fijaciones\\n    map_fix_frame = np.zeros((frame_h,frame_w))\\n    for ifix in range(fix.shape[0]):\\n        # Puede haber fijaciones fuera de los margenes de la imagen\\n        if(int(fix[ifix][0]) < frame_h and int(fix[ifix][1]) < frame_w):\\n            map_fix_frame[int(fix[ifix][1]), int(fix[ifix][0])] = 255   # Mapa de fijaciones\\n    # Convolucionamos con una gaussiana las fijaciones para obtener el mapa de densidad de fijaciones\\n    map_den_fix = (np.floor(cv2.normalize(cv2.GaussianBlur(np.float32(map_fix_frame),(85,85),20,20), None, 0, 255, cv2.NORM_MINMAX))).astype(np.uint8)\\n\\n    # Visualización\\n    cv2.imshow(\\'Saliency map {}\\'.format(randNum), map_den_fix)\\n    time.sleep(5)\\n\\n    # Actualizamos\\n    prev_frame = current_frame\\n\\n    if cv2.waitKey(1) & 0xFF == ord(\\'q\\'):\\n        break'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from IPython.display import display, Image\n",
    "\n",
    "def PDZhou(prev_frame, current_frame):\n",
    "    # Si el video está en escala de grises\n",
    "    if len(prev_frame) == 2:\n",
    "        # Calculamos la Transformada de Fourier 2D para cada fotograma\n",
    "        FFT1 = np.fft.fft2(prev_frame)\n",
    "        FFT2 = np.fft.fft2(current_frame)\n",
    "\n",
    "        # Calculamos la magnitud y la fase de cada transformada de Fourier\n",
    "        Amp1 = np.abs(FFT1)\n",
    "        Amp2 = np.abs(FFT2)\n",
    "        Phase1 = np.angle(FFT1)\n",
    "        Phase2 = np.angle(FFT2)\n",
    "\n",
    "        # Calculamos los mapas de magnitud\n",
    "        mMap1 = np.abs(np.fft.ifft2((Amp2 - Amp1) * np.exp(1j * Phase1)))\n",
    "        mMap2 = np.abs(np.fft.ifft2((Amp2 - Amp1) * np.exp(1j * Phase2)))\n",
    "\n",
    "        # Calculamos el mapa combinado\n",
    "        mMap = mMap1 * mMap2\n",
    "\n",
    "        # Suavizado\n",
    "        mMap = cv2.GaussianBlur(mMap,(85,85),20,20)\n",
    "\n",
    "    # Si tiene canales de color\n",
    "    else:\n",
    "        # Obtenemos los canales de color\n",
    "        prev_frame_channels = [prev_frame[:, :, 0], prev_frame[:, :, 1], prev_frame[:, :, 2]]\n",
    "        current_frame_channels = [current_frame[:, :, 0], current_frame[:, :, 1], current_frame[:, :, 2]]\n",
    "        results = []\n",
    "\n",
    "        # Calculamos el mapa de saliencia para cada canal de color\n",
    "        for i in range(len(prev_frame_channels)):\n",
    "            # Calculamos la Transformada de Fourier 2D para cada fotograma\n",
    "            FFT1 = np.fft.fft2(prev_frame_channels[i])\n",
    "            FFT2 = np.fft.fft2(current_frame_channels[i])\n",
    "\n",
    "            # Calculamos la magnitud y la fase de cada transformada de Fourier\n",
    "            Amp1 = np.abs(FFT1)\n",
    "            Amp2 = np.abs(FFT2)\n",
    "            Phase1 = np.angle(FFT1)\n",
    "            Phase2 = np.angle(FFT2)\n",
    "\n",
    "            # Calculamos los mapas de magnitud\n",
    "            mMap1 = np.abs(np.fft.ifft2((Amp2 - Amp1) * np.exp(1j * Phase1)))\n",
    "            mMap2 = np.abs(np.fft.ifft2((Amp2 - Amp1) * np.exp(1j * Phase2)))\n",
    "\n",
    "            # Calculamos el mapa combinado\n",
    "            mMap_channel = mMap1 * mMap2\n",
    "\n",
    "            # Suavizado\n",
    "            mMap_channel = cv2.GaussianBlur(mMap_channel,(85,85),20,20)\n",
    "\n",
    "            # Agregamos el mapa de saliencia del canal actual a la lista\n",
    "            results.append(mMap_channel)\n",
    "\n",
    "        # Sumamos los mapas de saliencia de cada canal de color\n",
    "        mMap = np.sum(results, axis=0)\n",
    "\n",
    "    # Normalización\n",
    "    mMap_norm = cv2.normalize(mMap, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "    return mMap_norm\n",
    "\n",
    "\n",
    "\n",
    "def normalize_map(s_map):\n",
    "\t# normalize the salience map\n",
    "\tnorm_s_map = (s_map - np.min(s_map))/((np.max(s_map)-np.min(s_map))*1.0)\n",
    "\treturn norm_s_map\n",
    "\n",
    "def discretize_gt(gt):\n",
    "\treturn gt/255\n",
    "\n",
    "def AUC_judd(s_map,gt):\n",
    "\t# ground truth is discrete, s_map is continous and normalized\n",
    "\tgt = discretize_gt(gt)\n",
    "\ts_map = normalize_map(s_map)\n",
    "\t# thresholds are calculated from the salience map, only at places where fixations are present\n",
    "\tthresholds = []\n",
    "\tfor i in range(0,gt.shape[0]):\n",
    "\t\tfor k in range(0,gt.shape[1]):\n",
    "\t\t\tif gt[i][k]>0:\n",
    "\t\t\t\tthresholds.append(s_map[i][k])\n",
    "\n",
    "\tnum_fixations = np.sum(gt)\n",
    "\t# num fixations is no. of salience map values at gt >0\n",
    "\tthresholds = sorted(set(thresholds))\n",
    "\t#fp_list = []\n",
    "\t#tp_list = []\n",
    "\tarea = []\n",
    "\tarea.append((0.0,0.0))\n",
    "\tfor thresh in thresholds:\n",
    "\t\t# in the salience map, keep only those pixels with values above threshold\n",
    "\t\ttemp = np.zeros(s_map.shape)\n",
    "\t\ttemp[s_map>=thresh] = 1.0\n",
    "\t\tnum_overlap = np.where(np.add(temp,gt)==2)[0].shape[0]\n",
    "\t\ttp = num_overlap/(num_fixations*1.0)\n",
    "\t\t\n",
    "\t\t# total number of pixels > threshold - number of pixels that overlap with gt / total number of non fixated pixels\n",
    "\t\t# this becomes nan when gt is full of fixations..this won't happen\n",
    "\t\tfp = (np.sum(temp) - num_overlap)/((np.shape(gt)[0] * np.shape(gt)[1]) - num_fixations)\n",
    "\t\t\n",
    "\t\tarea.append((round(tp,4),round(fp,4)))\n",
    "\t\t#tp_list.append(tp)\n",
    "\t\t#fp_list.append(fp)\n",
    "\n",
    "\t#tp_list.reverse()\n",
    "\t#fp_list.reverse()\n",
    "\tarea.append((1.0,1.0))\n",
    "\t#tp_list.append(1.0)\n",
    "\t#fp_list.append(1.0)\n",
    "\t#print tp_list\n",
    "\tarea.sort(key = lambda x:x[0])\n",
    "\ttp_list =  [x[0] for x in area]\n",
    "\tfp_list =  [x[1] for x in area]\n",
    "\treturn np.trapz(np.array(tp_list),np.array(fp_list))\n",
    "\n",
    "def AUC_shuff(s_map,gt,other_map,splits=100,stepsize=0.1):\n",
    "\tgt = discretize_gt(gt)\n",
    "\tother_map = discretize_gt(other_map)\n",
    "\ts_map = normalize_map(s_map)\n",
    "\n",
    "\tnum_fixations = np.sum(gt)\n",
    "\t\n",
    "\tx,y = np.where(other_map==1)\n",
    "\tother_map_fixs = []\n",
    "\tfor j in zip(x,y):\n",
    "\t\tother_map_fixs.append(j[0]*other_map.shape[0] + j[1])\n",
    "\tind = len(other_map_fixs)\n",
    "\tassert ind == len(other_map_fixs), 'something is wrong in auc shuffle'\n",
    "\n",
    "\n",
    "\tnum_fixations_other = min(ind,num_fixations)\n",
    "\n",
    "\tnum_pixels = s_map.shape[0]*s_map.shape[1]\n",
    "\trandom_numbers = []\n",
    "\tfor i in range(0,splits):\n",
    "\t\ttemp_list = []\n",
    "\t\tt1 = np.random.permutation(ind)\n",
    "\t\tfor k in t1:\n",
    "\t\t\ttemp_list.append(other_map_fixs[k])\n",
    "\t\trandom_numbers.append(temp_list)\t\n",
    "\n",
    "\taucs = []\n",
    "\t# for each split, calculate auc\n",
    "\tfor i in random_numbers:\n",
    "\t\tr_sal_map = []\n",
    "\t\tfor k in i:\n",
    "\t\t\t#print('{}  {}'.format(k%s_map.shape[0]-1, int(np.floor(k/s_map.shape[0]))))\n",
    "\t\t\tr_sal_map.append(s_map[k%s_map.shape[0]-1, int(np.floor(k/s_map.shape[0]))])\n",
    "\t\t# in these values, we need to find thresholds and calculate auc\n",
    "\t\tthresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "\t\tr_sal_map = np.array(r_sal_map)\n",
    "\n",
    "\t\t# once threshs are got\n",
    "\t\tthresholds = sorted(set(thresholds))\n",
    "\t\tarea = []\n",
    "\t\tarea.append((0.0,0.0))\n",
    "\t\tfor thresh in thresholds:\n",
    "\t\t\t# in the salience map, keep only those pixels with values above threshold\n",
    "\t\t\ttemp = np.zeros(s_map.shape)\n",
    "\t\t\ttemp[s_map>=thresh] = 1.0\n",
    "\t\t\tnum_overlap = np.where(np.add(temp,gt)==2)[0].shape[0]\n",
    "\t\t\ttp = num_overlap/(num_fixations*1.0)\n",
    "\t\t\t\n",
    "\t\t\t#fp = (np.sum(temp) - num_overlap)/((np.shape(gt)[0] * np.shape(gt)[1]) - num_fixations)\n",
    "\t\t\t# number of values in r_sal_map, above the threshold, divided by num of random locations = num of fixations\n",
    "\t\t\tfp = len(np.where(r_sal_map>thresh)[0])/(num_fixations*1.0)\n",
    "\n",
    "\t\t\tarea.append((round(tp,4),round(fp,4)))\n",
    "\t\t\n",
    "\t\tarea.append((1.0,1.0))\n",
    "\t\tarea.sort(key = lambda x:x[0])\n",
    "\t\ttp_list =  [x[0] for x in area]\n",
    "\t\tfp_list =  [x[1] for x in area]\n",
    "\n",
    "\t\taucs.append(np.trapz(np.array(tp_list),np.array(fp_list)))\n",
    "\t\n",
    "\treturn np.mean(aucs)\n",
    "\n",
    "def NSS(s_map,gt):\n",
    "\tgt = discretize_gt(gt)\n",
    "\ts_map_norm = (s_map - np.mean(s_map))/np.std(s_map)\n",
    "\n",
    "\tx,y = np.where(gt==1)\n",
    "\ttemp = []\n",
    "\tfor i in zip(x,y):\n",
    "\t\ttemp.append(s_map_norm[i[0],i[1]])\n",
    "\treturn np.mean(temp)\n",
    "\n",
    "\n",
    "def CC(s_map,gt):\n",
    "\ts_map_norm = (s_map - np.mean(s_map))/np.std(s_map)\n",
    "\tgt_norm = (gt - np.mean(gt))/np.std(gt)\n",
    "\ta = s_map_norm\n",
    "\tb= gt_norm\n",
    "\tr = (a*b).sum() / np.sqrt((a*a).sum() * (b*b).sum())\n",
    "\treturn r\n",
    "\n",
    "\n",
    "# Definimos el path a los videos\n",
    "videos_path = './videos/'\n",
    "\n",
    "# Cargamos los mapas de fijación\n",
    "fixation_maps =  sio.loadmat('./FixVideoData.mat')\n",
    "\n",
    "# Definimos el número de videos a procesar\n",
    "nvideos = 6\n",
    "\n",
    "# Recuperamos as propieades do video.\n",
    "frame_w = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_h = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "frame_count = 0\n",
    "for ivideo in range(fixation_maps['EyeTrackVDB'].shape[0]):\n",
    "\tname_video = fixation_maps['EyeTrackVDB'][ivideo]['Name'][0][0]\n",
    "\tvideo = cv2.VideoCapture(videos_path + name_video)\n",
    "\n",
    "\tprint(len(fixation_maps['EyeTrackVDB'][ivideo]['Frame'][0]['FixL'][0]))\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\t# count the number of frames \n",
    "\tframes = video.get(cv2.CAP_PROP_FRAME_COUNT) \n",
    "\tret, frame = video.read()\n",
    "\tprint(frames)\n",
    "\"\"\"ret, prev_frame = video.read()\n",
    "while(True):\n",
    "    ret, current_frame = video.read()\n",
    "    if(not ret):\n",
    "        break\n",
    "\n",
    "    # Obtenemos el mapa de saliencia para el frame\n",
    "    #gp_sm_frame = PFTVideo_Color_Multiescala(prev_frame, current_frame)  # Fase global\n",
    "    #pd_sm_frame = PDZhou(prev_frame, current_frame)  # Discrepancia de la fase\n",
    "\n",
    "    # Leer las fijaciones del frame\n",
    "    fix = fixation_maps['EyeTrackVDB'][0]['Frame'][0]['FixL'][0,frame_count][:,0:2]\n",
    "    \n",
    "    print(fixation_maps['EyeTrackVDB'][0]['Frame'][0]['FixL'][0,frame_count][:,0:2])\n",
    "\n",
    "    randNum = np.random.randint(len(fixation_maps['EyeTrackVDB'][ivideo]['Frame'][0]['FixL'][0]))\n",
    "    print(randNum)\n",
    "    other_fix = fixation_maps['EyeTrackVDB'][0]['Frame'][0]['FixL'][0,randNum][:,0:2]\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\"\"\"\n",
    "\n",
    "\"\"\"if (video.isOpened() == False):\n",
    "    print(\"Error en la apertura del fichero {}\".format(videos_path + name_video))\n",
    "\n",
    "# Recuperamos as propieades do video.\n",
    "frame_w = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_h = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "frame_count = 0\n",
    "ret, prev_frame = video.read()\n",
    "while(True):\n",
    "    ret, current_frame = video.read()\n",
    "    if(not ret):\n",
    "        break\n",
    "\n",
    "    # Obtenemos el mapa de saliencia para el frame\n",
    "    #gp_sm_frame = PFTVideo_Color_Multiescala(prev_frame, current_frame)  # Fase global\n",
    "    pd_sm_frame = PDZhou(prev_frame, current_frame)  # Discrepancia de la fase\n",
    "\n",
    "    # Leer las fijaciones del frame\n",
    "    fix = fixation_maps['EyeTrackVDB'][ivideo]['Frame'][0]['FixL'][0,frame_count][:,0:2]\n",
    "    randNum = np.random.randint(len(fixation_maps['EyeTrackVDB'][ivideo]['Frame'][0]['FixL'][0]))\n",
    "    print(randNum)\n",
    "    other_fix = fixation_maps['EyeTrackVDB'][ivideo]['Frame'][0]['FixL'][0,randNum][:,0:2]\n",
    "\n",
    "    # Construir el mapa de densidad de fijaciones\n",
    "    map_fix_frame = np.zeros((frame_h,frame_w))\n",
    "    for ifix in range(fix.shape[0]):\n",
    "        # Puede haber fijaciones fuera de los margenes de la imagen\n",
    "        if(int(fix[ifix][0]) < frame_h and int(fix[ifix][1]) < frame_w):\n",
    "            map_fix_frame[int(fix[ifix][1]), int(fix[ifix][0])] = 255   # Mapa de fijaciones\n",
    "    # Convolucionamos con una gaussiana las fijaciones para obtener el mapa de densidad de fijaciones\n",
    "    map_den_fix = (np.floor(cv2.normalize(cv2.GaussianBlur(np.float32(map_fix_frame),(85,85),20,20), None, 0, 255, cv2.NORM_MINMAX))).astype(np.uint8)\n",
    "\n",
    "    # Visualización\n",
    "    cv2.imshow('Saliency map {}'.format(randNum), map_den_fix)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Actualizamos\n",
    "    prev_frame = current_frame\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\"\"\"\n",
    "\n",
    "# Liberar recursos\n",
    "#cap.release()\n",
    "#cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VAA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "53e3713d2492df86f3f230a9890fee7d1a4758cc3b2a50c4b61b1b972bfca11c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
